{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chirayu-khandelwal/Parkinson_Detection/blob/main/PHD_PD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-phd-intro",
      "metadata": {
        "id": "markdown-phd-intro"
      },
      "source": [
        "# PhD-Level Approach: Maximizing Parkinson's Severity Prediction Performance\n",
        "\n",
        "**Objective:** To rigorously explore and optimize machine learning methodologies for predicting `motor_UPDRS` score using the UCI Parkinson's Telemonitoring dataset, employing advanced techniques in feature engineering, modeling, validation, and interpretation.\n",
        "\n",
        "**Methodology:** This notebook outlines a comprehensive, research-oriented workflow, acknowledging the limitation of using pre-extracted features instead of raw audio signals.\n",
        "\n",
        "**Workflow Overview:**\n",
        "1.  **Setup & Literature Review:** Define environment, tools, and review relevant studies.\n",
        "2.  **Deep Data Exploration & Understanding:** Go beyond basic EDA.\n",
        "3.  **Advanced Preprocessing & Cleaning:** Robust handling of potential issues.\n",
        "4.  **Extensive Feature Engineering & Exploration:** Explore diverse feature types beyond basic FFT.\n",
        "5.  **Sophisticated Feature Selection & Dimensionality Reduction:** Select the most informative feature subset.\n",
        "6.  **Rigorous Modeling Strategy & Validation Design:** Plan advanced models and robust validation (Nested CV).\n",
        "7.  **Advanced Hyperparameter Optimization:** Employ efficient search strategies (e.g., Bayesian Optimization).\n",
        "8.  **Model Training & Evaluation (Nested CV):** Execute the defined strategy.\n",
        "9.  **Ensemble Methods & Final Model Selection:** Combine strong models.\n",
        "10. **Model Interpretation & Explainability:** Understand *why* the model predicts.\n",
        "11. **Reporting, Discussion & Future Work:** Document findings, limitations, and next steps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-phd-step1",
      "metadata": {
        "id": "markdown-phd-step1"
      },
      "source": [
        "## 1. Setup & Literature Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-phd-step1-setup",
      "metadata": {
        "id": "code-phd-step1-setup"
      },
      "outputs": [],
      "source": [
        "# --- Core Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # Suppress routine warnings\n",
        "\n",
        "# --- Data Handling ---\n",
        "# Install ucimlrepo if not already installed\n",
        "try:\n",
        "    import ucimlrepo\n",
        "except ImportError:\n",
        "    print(\"Installing ucimlrepo...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ucimlrepo\"])\n",
        "    import ucimlrepo\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# --- Preprocessing & Feature Engineering ---\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PolynomialFeatures\n",
        "from sklearn.impute import KNNImputer # More advanced imputation\n",
        "from numpy.fft import fft, fftfreq\n",
        "\n",
        "!pip install PyWavelets\n",
        "import pywt # For Wavelet Transforms\n",
        "\n",
        "from scipy import stats # For statistical features\n",
        "\n",
        "# --- Feature Selection ---\n",
        "from sklearn.feature_selection import RFE, SelectFromModel, mutual_info_regression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LassoCV # Lasso for feature selection\n",
        "\n",
        "# --- Modeling ---\n",
        "# Standard\n",
        "from sklearn.linear_model import RidgeCV, ElasticNetCV\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
        "\n",
        "!pip install xgboost\n",
        "import xgboost as xgb\n",
        "\n",
        "!pip install lightgbm\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Deep Learning\n",
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "# Other Advanced\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
        "\n",
        "# --- Validation & Hyperparameter Tuning ---\n",
        "from sklearn.model_selection import KFold, GroupKFold, cross_val_score, cross_validate, RandomizedSearchCV, GridSearchCV\n",
        "!pip install scikit-optimize # Or hyperopt, optuna\n",
        "from skopt import BayesSearchCV # Example: Bayesian Optimization\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "\n",
        "# --- Ensemble Methods ---\n",
        "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
        "\n",
        "# --- Evaluation & Interpretation ---\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
        "# !pip install shap\n",
        "import shap\n",
        "\n",
        "# --- Configuration ---\n",
        "SEED = 42\n",
        "N_SPLITS_OUTER = 5 # For outer cross-validation loop\n",
        "N_SPLITS_INNER = 3 # For inner hyperparameter tuning loop (within outer loop)\n",
        "N_ITER_BAYES = 50 # Number of iterations for Bayesian optimization\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"Libraries imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-phd-step1b",
      "metadata": {
        "id": "markdown-phd-step1b"
      },
      "source": [
        "### 1b. Literature Review (Conceptual)\n",
        "\n",
        "*   **Goal:** Understand state-of-the-art methods for UPDRS prediction specifically from *voice features* similar to those in this dataset.\n",
        "*   **Keywords:** Parkinson's UPDRS prediction, voice features, machine learning, regression, jitter, shimmer, NHR, HNR, dysphonia.\n",
        "*   **Sources:** Google Scholar, PubMed, IEEE Xplore, arXiv.\n",
        "*   **Key Questions:**\n",
        "    *   Which specific voice features are consistently found to be most predictive?\n",
        "    *   What feature engineering techniques (beyond basic stats) have been applied?\n",
        "    *   Which ML/DL models perform best?\n",
        "    *   What validation strategies are used (simple split, CV, subject-aware CV)?\n",
        "    *   What performance metrics (RMSE, RÂ², MAE) are typically reported?\n",
        "    *   What are the reported limitations and challenges?\n",
        "*   **Action:** Summarize findings to guide feature selection, model choice, and interpretation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-phd-step2",
      "metadata": {
        "id": "markdown-phd-step2"
      },
      "source": [
        "## 2. Deep Data Exploration & Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-phd-step2-load",
      "metadata": {
        "id": "code-phd-step2-load"
      },
      "outputs": [],
      "source": [
        "print(\"--- 2a. Loading Data ---\")\n",
        "try:\n",
        "    parkinsons_telemonitoring = fetch_ucirepo(id=189)\n",
        "    df_full = parkinsons_telemonitoring.data.original # Get original df which might include subject#\n",
        "    if df_full is None:\n",
        "       print(\"Original dataframe not available, fetching features/targets separately.\")\n",
        "       X_data = parkinsons_telemonitoring.data.features\n",
        "       y_data = parkinsons_telemonitoring.data.targets\n",
        "       df_full = pd.concat([X_data, y_data], axis=1)\n",
        "    print(\"Dataset loaded.\")\n",
        "    # print(parkinsons_telemonitoring.metadata)\n",
        "    # print(parkinsons_telemonitoring.variables)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}. Attempting local CSV.\")\n",
        "    try:\n",
        "        df_full = pd.read_csv('data.csv')\n",
        "        print(\"Loaded from data.csv\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"ERROR: data.csv not found.\")\n",
        "        raise\n",
        "\n",
        "target_variable = 'motor_UPDRS'\n",
        "if target_variable not in df_full.columns:\n",
        "    raise ValueError(f\"Target variable '{target_variable}' not found.\")\n",
        "\n",
        "print(f\"Full dataset shape: {df_full.shape}\")\n",
        "display(df_full.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-phd-step2-eda",
      "metadata": {
        "id": "code-phd-step2-eda"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- 2b. Deep Exploration ---\")\n",
        "\n",
        "# 1. Basic Info\n",
        "print(\"\\nBasic Info:\")\n",
        "df_full.info()\n",
        "\n",
        "# 2. Descriptive Statistics\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "display(df_full.describe().T)\n",
        "\n",
        "# 3. Missing Values (Re-check)\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df_full.isna().sum())\n",
        "# If missing values are found, consider advanced imputation like KNNImputer later.\n",
        "\n",
        "# 4. Duplicate Rows\n",
        "print(\"\\nDuplicate Rows:\")\n",
        "print(f\"Number of duplicate rows: {df_full.duplicated().sum()}\")\n",
        "# Consider implications: Are duplicates expected? Errors?\n",
        "# df_full = df_full.drop_duplicates()\n",
        "\n",
        "# 5. Target Variable Distribution\n",
        "print(f\"\\nTarget Variable ({target_variable}) Distribution:\")\n",
        "sns.histplot(df_full[target_variable], kde=True)\n",
        "plt.title(f'Distribution of {target_variable}')\n",
        "plt.show()\n",
        "sns.boxplot(x=df_full[target_variable])\n",
        "plt.title(f'Boxplot of {target_variable}')\n",
        "plt.show()\n",
        "print(f\"Skewness: {df_full[target_variable].skew():.2f}\")\n",
        "# Consider potential transformations (log, Box-Cox) if highly skewed, though often not needed for tree models.\n",
        "\n",
        "# 6. Feature Distributions & Outliers\n",
        "print(\"\\nFeature Distributions (Example - first few numerical):\")\n",
        "numerical_features = df_full.select_dtypes(include=np.number).drop(columns=[target_variable, 'total_UPDRS'], errors='ignore')\n",
        "if 'subject#' in numerical_features.columns: numerical_features = numerical_features.drop(columns=['subject#'])\n",
        "\n",
        "plt.figure(figsize=(15, min(5*len(numerical_features.columns)//4, 30)))\n",
        "for i, col in enumerate(numerical_features.columns[:min(len(numerical_features.columns), 16)]): # Plot first N features\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    sns.histplot(numerical_features[col], kde=True, bins=30)\n",
        "    plt.title(col)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# Consider outlier detection methods (IQR, Z-score, Isolation Forest) and handling strategies.\n",
        "\n",
        "# 7. Correlations\n",
        "print(\"\\nCorrelation Analysis:\")\n",
        "plt.figure(figsize=(18, 15))\n",
        "corr_matrix = df_full.drop(columns=['subject#'], errors='ignore').corr()\n",
        "sns.heatmap(corr_matrix, cmap='coolwarm', annot=False, fmt='.1f') # Annot=True is too dense\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "target_corr = corr_matrix[[target_variable]].sort_values(by=target_variable, ascending=False)\n",
        "plt.figure(figsize=(6, 10))\n",
        "sns.heatmap(target_corr, annot=True, cmap='viridis', fmt='.2f')\n",
        "plt.title(f'Feature Correlation with {target_variable}')\n",
        "plt.show()\n",
        "# Note high correlations between features (multicollinearity) - may affect linear models.\n",
        "\n",
        "# 8. Subject Analysis (if 'subject#' exists and is reliable)\n",
        "if 'subject#' in df_full.columns:\n",
        "    print(\"\\nSubject Analysis:\")\n",
        "    n_subjects = df_full['subject#'].nunique()\n",
        "    print(f\"Number of unique subjects: {n_subjects}\")\n",
        "    subject_counts = df_full['subject#'].value_counts()\n",
        "    print(\"Records per subject (summary):\")\n",
        "    print(subject_counts.describe())\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.histplot(subject_counts, bins=30)\n",
        "    plt.title('Distribution of Records per Subject')\n",
        "    plt.xlabel('Number of Records')\n",
        "    plt.show()\n",
        "    # This unequal distribution is important for validation (GroupKFold).\n",
        "\n",
        "    # Check variation of target within/between subjects (example)\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    sns.boxplot(x='subject#', y=target_variable, data=df_full, order=subject_counts.index[:min(n_subjects, 20)]) # Show first ~20 subjects\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.title(f'{target_variable} Distribution Across Subjects (Sample)')\n",
        "    plt.show()\n",
        "    # Significant variability suggests subject ID could be a useful feature or grouping factor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-phd-step3",
      "metadata": {
        "id": "markdown-phd-step3"
      },
      "source": [
        "## 3. Advanced Preprocessing & Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-phd-step3-preprocess",
      "metadata": {
        "id": "code-phd-step3-preprocess"
      },
      "outputs": [],
      "source": [
        "print(\"--- 3. Advanced Preprocessing ---\")\n",
        "\n",
        "# Define Features (X) and Target (y) for modeling\n",
        "y = df_full[target_variable].values\n",
        "X = df_full.drop(columns=[target_variable, 'total_UPDRS'], errors='ignore') # Drop both targets\n",
        "groups = None # For GroupKFold if subject ID is used\n",
        "if 'subject#' in X.columns:\n",
        "    print(\"Using 'subject#' for grouped cross-validation.\")\n",
        "    groups = X['subject#'].values\n",
        "    X = X.drop(columns=['subject#']) # Drop identifier after extracting groups\n",
        "else:\n",
        "    print(\"Warning: 'subject#' column not found. Standard KFold will be used.\")\n",
        "\n",
        "feature_names_initial = X.columns.tolist()\n",
        "print(f\"Shape of X before further processing: {X.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")\n",
        "\n",
        "# --- Outlier Handling (Example using IQR - apply cautiously) ---\n",
        "# This step is complex and domain-knowledge dependent. Excessive removal can harm performance.\n",
        "# Consider robust models or robust scaling instead.\n",
        "# Example (conceptual - would need careful implementation within CV):\n",
        "# for col in X.columns:\n",
        "#     Q1 = X[col].quantile(0.25)\n",
        "#     Q3 = X[col].quantile(0.75)\n",
        "#     IQR = Q3 - Q1\n",
        "#     lower_bound = Q1 - 1.5 * IQR\n",
        "#     upper_bound = Q3 + 1.5 * IQR\n",
        "#     # Option 1: Cap outliers\n",
        "#     # X[col] = np.clip(X[col], lower_bound, upper_bound)\n",
        "#     # Option 2: Mark outliers (requires model that can handle NaN or specific value)\n",
        "#     # X[col+'_is_outlier'] = ((X[col] < lower_bound) | (X[col] > upper_bound)).astype(int)\n",
        "print(\"Outlier handling strategy: To be applied carefully, potentially using RobustScaler or within CV.\")\n",
        "\n",
        "# --- Advanced Imputation (Example: KNNImputer if missing values exist) ---\n",
        "if X.isna().sum().sum() > 0:\n",
        "   print(\"Applying KNNImputer...\")\n",
        "   imputer = KNNImputer(n_neighbors=5)\n",
        "   X_imputed = imputer.fit_transform(X)\n",
        "   X = pd.DataFrame(X_imputed, columns=feature_names_initial)\n",
        "else:\n",
        "   print(\"No missing values found, skipping imputation.\")\n",
        "\n",
        "# --- Scaling Strategy ---\n",
        "# Scaling will be applied *within* the cross-validation loop to prevent data leakage.\n",
        "# Options: MinMaxScaler, StandardScaler, RobustScaler (good for outliers).\n",
        "print(\"Scaling: To be performed within cross-validation using StandardScaler or RobustScaler.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-phd-step4",
      "metadata": {
        "id": "markdown-phd-step4"
      },
      "source": [
        "## 4. Extensive Feature Engineering & Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-phd-step4-fe",
      "metadata": {
        "id": "code-phd-step4-fe"
      },
      "outputs": [],
      "source": [
        "print(\"--- 4. Feature Engineering (Conceptual - apply within CV) ---\")\n",
        "\n",
        "# NOTE: Feature engineering should ideally be done *inside* the cross-validation loop\n",
        "#       on the training fold only to prevent data leakage. This section defines the functions.\n",
        "\n",
        "def engineer_features(X_train_fold, X_test_fold):\n",
        "    \"\"\"Applies various feature engineering techniques.\"\"\"\n",
        "    X_train_eng = X_train_fold.copy()\n",
        "    X_test_eng = X_test_fold.copy()\n",
        "    original_cols = X_train_fold.columns.tolist()\n",
        "\n",
        "    # --- 4a. Polynomial / Interaction Features ---\n",
        "    # Caution: Can lead to very high dimensionality\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
        "    # Fit only on training data\n",
        "    poly.fit(X_train_eng[original_cols])\n",
        "    X_train_poly = poly.transform(X_train_eng[original_cols])\n",
        "    X_test_poly = poly.transform(X_test_eng[original_cols])\n",
        "    poly_feature_names = poly.get_feature_names_out(original_cols)\n",
        "    X_train_eng = pd.DataFrame(X_train_poly, columns=poly_feature_names, index=X_train_eng.index)\n",
        "    X_test_eng = pd.DataFrame(X_test_poly, columns=poly_feature_names, index=X_test_eng.index)\n",
        "    print(f\"Shape after PolynomialFeatures: {X_train_eng.shape}\")\n",
        "\n",
        "    # --- 4b. Statistical Features (Examples) ---\n",
        "    # Rolling window features could be attempted if data had a reliable time order, but likely not applicable here.\n",
        "    # Higher-order moments (applied per feature - less common unless theoretically justified)\n",
        "    # for col in original_cols:\n",
        "    #    X_train_eng[f'{col}_skew'] = stats.skew(X_train_eng[col]) # This is a single value, not row-wise\n",
        "    #    X_test_eng[f'{col}_skew'] = stats.skew(X_test_eng[col]) # Need careful application\n",
        "\n",
        "    # --- 4c. Frequency Domain Features (FFT Variants) ---\n",
        "    # Apply FFT to each column *signal* across the fold (as done before, but inside CV)\n",
        "    # Consider: Energy in frequency bands, spectral entropy, phase info (complex)\n",
        "    n_fft_components = 5\n",
        "    for col in original_cols:\n",
        "        # Fit FFT summary stats ON TRAINING FOLD ONLY\n",
        "        signal_train = X_train_eng[col].values\n",
        "        N_train = len(signal_train)\n",
        "        if N_train <= 1: continue\n",
        "        yf_train = fft(signal_train)\n",
        "        yf_mag_train = np.abs(yf_train[:N_train//2])\n",
        "        if len(yf_mag_train) > 0:\n",
        "            actual_n = min(n_fft_components, len(yf_mag_train))\n",
        "            top_indices = np.argsort(yf_mag_train)[::-1][:actual_n]\n",
        "            top_magnitudes = yf_mag_train[top_indices]\n",
        "            padded_mags = np.pad(top_magnitudes, (0, n_fft_components - len(top_magnitudes)), 'constant') if len(top_magnitudes) < n_fft_components else top_magnitudes\n",
        "            for i in range(n_fft_components):\n",
        "                # Add the *summary* stat (calculated on train) to both train and test folds\n",
        "                fft_val = padded_mags[i]\n",
        "                X_train_eng[f'{col}_fft_mag_{i+1}'] = fft_val\n",
        "                X_test_eng[f'{col}_fft_mag_{i+1}'] = fft_val # Use train-fold stat for test fold\n",
        "        else:\n",
        "             for i in range(n_fft_components):\n",
        "                 X_train_eng[f'{col}_fft_mag_{i+1}'] = 0\n",
        "                 X_test_eng[f'{col}_fft_mag_{i+1}'] = 0\n",
        "\n",
        "    # --- 4d. Time-Frequency Domain Features (Wavelets) ---\n",
        "    # Apply DWT to each column *signal* (similar limitations as FFT)\n",
        "    # Example: Extract energy from DWT coefficients\n",
        "    # wavelet = 'db4' # Example wavelet\n",
        "    # for col in original_cols:\n",
        "    #    coeffs_train = pywt.wavedec(X_train_eng[col], wavelet, level=3) # Example level\n",
        "    #    # Extract features from coeffs_train (e.g., energy, entropy)\n",
        "    #    # Apply the same *type* of feature extraction conceptually to test set\n",
        "    #    # This requires careful thought on how to handle train/test consistency\n",
        "    print(\"Wavelet feature engineering: Conceptually possible but needs careful implementation within CV.\")\n",
        "\n",
        "    # --- 4e. Domain-Specific Features (Based on Literature Review) ---\n",
        "    # Example: Calculate ratios identified as important, e.g., Shimmer/Jitter ratio\n",
        "    # if 'Shimmer(%)' in X_train_eng.columns and 'Jitter(%)' in X_train_eng.columns:\n",
        "    #    X_train_eng['Shim_Jit_Ratio'] = X_train_eng['Shimmer(%)'] / (X_train_eng['Jitter(%)'] + 1e-6)\n",
        "    #    X_test_eng['Shim_Jit_Ratio'] = X_test_eng['Shimmer(%)'] / (X_test_eng['Jitter(%)'] + 1e-6)\n",
        "\n",
        "    # --- Final Check ---\n",
        "    # Ensure no NaN/inf values introduced\n",
        "    X_train_eng.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    X_test_eng.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    # Impute any NaNs possibly created during engineering (using mean/median fitted on train)\n",
        "    for col in X_train_eng.columns:\n",
        "        if X_train_eng[col].isna().any():\n",
        "            fill_value = X_train_eng[col].median()\n",
        "            X_train_eng[col].fillna(fill_value, inplace=True)\n",
        "            X_test_eng[col].fillna(fill_value, inplace=True) # Use train median for test\n",
        "\n",
        "    return X_train_eng, X_test_eng\n",
        "\n",
        "# Conceptual call - Actual call happens inside CV loop\n",
        "# X_train_sample = pd.DataFrame(np.random.rand(100, len(feature_names_initial)), columns=feature_names_initial)\n",
        "# X_test_sample = pd.DataFrame(np.random.rand(20, len(feature_names_initial)), columns=feature_names_initial)\n",
        "# X_train_eng_sample, X_test_eng_sample = engineer_features(X_train_sample, X_test_sample)\n",
        "# print(f\"Sample engineered train shape: {X_train_eng_sample.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-phd-step5",
      "metadata": {
        "id": "markdown-phd-step5"
      },
      "source": [
        "## 5. Sophisticated Feature Selection & Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-phd-step5-fs",
      "metadata": {
        "id": "code-phd-step5-fs"
      },
      "outputs": [],
      "source": [
        "print(\"--- 5. Feature Selection / Dimensionality Reduction (Conceptual - apply within CV) ---\")\n",
        "\n",
        "# NOTE: This step follows Feature Engineering and precedes Scaling/Modeling inside the CV loop.\n",
        "#       It must be fitted *only* on the training fold.\n",
        "\n",
        "def select_features(X_train_fold, y_train_fold, X_test_fold, method='lasso', n_features=50):\n",
        "    \"\"\"Selects features using a specified method.\"\"\"\n",
        "    print(f\"Applying feature selection method: {method}...\")\n",
        "\n",
        "    if method == 'lasso':\n",
        "        # Use LassoCV to find optimal alpha and select features\n",
        "        # Scale data first for Lasso\n",
        "        scaler_fs = StandardScaler().fit(X_train_fold)\n",
        "        X_train_scaled = scaler_fs.transform(X_train_fold)\n",
        "        X_test_scaled = scaler_fs.transform(X_test_fold)\n",
        "\n",
        "        selector = SelectFromModel(LassoCV(cv=3, random_state=SEED, n_jobs=-1), threshold=1e-5) # Small threshold\n",
        "        selector.fit(X_train_scaled, y_train_fold)\n",
        "        X_train_selected = selector.transform(X_train_scaled)\n",
        "        X_test_selected = selector.transform(X_test_scaled)\n",
        "        selected_mask = selector.get_support()\n",
        "        selected_features = X_train_fold.columns[selected_mask].tolist()\n",
        "        print(f\"Lasso selected {len(selected_features)} features.\")\n",
        "        # Return unscaled selected data\n",
        "        return X_train_fold[selected_features], X_test_fold[selected_features]\n",
        "\n",
        "    elif method == 'rfe':\n",
        "        # Recursive Feature Elimination with a base estimator (e.g., RandomForest)\n",
        "        # computationally expensive\n",
        "        estimator = RandomForestRegressor(n_estimators=50, random_state=SEED, n_jobs=-1)\n",
        "        selector = RFE(estimator, n_features_to_select=n_features, step=0.1) # Adjust step\n",
        "        selector.fit(X_train_fold, y_train_fold)\n",
        "        selected_mask = selector.get_support()\n",
        "        selected_features = X_train_fold.columns[selected_mask].tolist()\n",
        "        print(f\"RFE selected {len(selected_features)} features.\")\n",
        "        return X_train_fold[selected_features], X_test_fold[selected_features]\n",
        "\n",
        "    elif method == 'mutual_info':\n",
        "        mi = mutual_info_regression(X_train_fold, y_train_fold, random_state=SEED)\n",
        "        mi_series = pd.Series(mi, index=X_train_fold.columns).sort_values(ascending=False)\n",
        "        selected_features = mi_series.head(n_features).index.tolist()\n",
        "        print(f\"Mutual Info selected {len(selected_features)} features.\")\n",
        "        return X_train_fold[selected_features], X_test_fold[selected_features]\n",
        "\n",
        "    elif method == 'pca':\n",
        "        # Dimensionality reduction rather than selection\n",
        "        scaler_pca = StandardScaler().fit(X_train_fold)\n",
        "        X_train_scaled = scaler_pca.transform(X_train_fold)\n",
        "        X_test_scaled = scaler_pca.transform(X_test_fold)\n",
        "\n",
        "        pca = PCA(n_components=n_features) # Or variance explained e.g., 0.95\n",
        "        pca.fit(X_train_scaled)\n",
        "        X_train_selected = pca.transform(X_train_scaled)\n",
        "        X_test_selected = pca.transform(X_test_scaled)\n",
        "        print(f\"PCA applied, resulting shape: {X_train_selected.shape}\")\n",
        "        # Note: Feature names are lost ('PC1', 'PC2', ...)\n",
        "        return pd.DataFrame(X_train_selected), pd.DataFrame(X_test_selected)\n",
        "\n",
        "    else: # No selection\n",
        "        return X_train_fold, X_test_fold\n",
        "\n",
        "# Conceptual Call Example:\n",
        "# X_train_sel, X_test_sel = select_features(X_train_eng_sample, y_train_sample, X_test_eng_sample, method='lasso')\n",
        "# print(f\"Sample selected train shape: {X_train_sel.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-phd-step6",
      "metadata": {
        "id": "markdown-phd-step6"
      },
      "source": [
        "## 6. Rigorous Modeling Strategy & Validation Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-phd-step6-strategy",
      "metadata": {
        "id": "code-phd-step6-strategy"
      },
      "outputs": [],
      "source": [
        "print(\"--- 6. Modeling Strategy and Validation Design ---\")\n",
        "\n",
        "# --- Validation Strategy: Nested Cross-Validation ---\n",
        "# Outer loop: Estimates generalization performance unbiasedly.\n",
        "# Inner loop: Used for hyperparameter tuning for the model trained on the outer fold.\n",
        "if groups is not None:\n",
        "    # Use GroupKFold if subject IDs are available to prevent subject leakage\n",
        "    outer_cv = GroupKFold(n_splits=N_SPLITS_OUTER)\n",
        "    inner_cv = GroupKFold(n_splits=N_SPLITS_INNER) # Use GroupKFold here too if possible\n",
        "    cv_groups = groups\n",
        "    print(f\"Using GroupKFold with {N_SPLITS_OUTER} outer splits and {N_SPLITS_INNER} inner splits.\")\n",
        "else:\n",
        "    # Use standard KFold if no groups\n",
        "    outer_cv = KFold(n_splits=N_SPLITS_OUTER, shuffle=True, random_state=SEED)\n",
        "    inner_cv = KFold(n_splits=N_SPLITS_INNER, shuffle=True, random_state=SEED)\n",
        "    cv_groups = None # No groups passed to splitting method\n",
        "    print(f\"Using KFold with {N_SPLITS_OUTER} outer splits and {N_SPLITS_INNER} inner splits.\")\n",
        "\n",
        "# --- Scoring Metric ---\n",
        "# Use negative RMSE because optimization functions typically maximize\n",
        "scoring = make_scorer(lambda y_true, y_pred: -np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "print(f\"Primary scoring metric: Negative RMSE\")\n",
        "\n",
        "# --- Candidate Models & Parameter Spaces (Examples) ---\n",
        "models_to_evaluate = {\n",
        "    'Ridge': {\n",
        "        'model': RidgeCV(alphas=np.logspace(-4, 4, 9), cv=inner_cv), # RidgeCV does its own inner CV\n",
        "        'param_space': None # Handled by RidgeCV\n",
        "    },\n",
        "    'Lasso': {\n",
        "        'model': LassoCV(cv=inner_cv, random_state=SEED, n_jobs=-1, max_iter=2000),\n",
        "        'param_space': None\n",
        "    },\n",
        "    'SVR': {\n",
        "        'model': SVR(),\n",
        "        'param_space': {\n",
        "            'C': Real(1e-2, 1e3, prior='log-uniform'),\n",
        "            'gamma': Real(1e-4, 1e1, prior='log-uniform'),\n",
        "            'kernel': Categorical(['rbf']), # RBF is common, linear could be added\n",
        "            'epsilon': Real(0.01, 0.5, prior='uniform')\n",
        "        }\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'model': RandomForestRegressor(random_state=SEED, n_jobs=-1),\n",
        "        'param_space': {\n",
        "            'n_estimators': Integer(100, 1000),\n",
        "            'max_depth': Integer(5, 50),\n",
        "            'min_samples_split': Integer(2, 20),\n",
        "            'min_samples_leaf': Integer(1, 10),\n",
        "            'max_features': Real(0.1, 1.0, prior='uniform') # Fraction of features\n",
        "        }\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model': xgb.XGBRegressor(random_state=SEED, objective='reg:squarederror', n_jobs=-1),\n",
        "        'param_space': {\n",
        "            'n_estimators': Integer(100, 1000),\n",
        "            'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
        "            'max_depth': Integer(3, 15),\n",
        "            'subsample': Real(0.5, 1.0, prior='uniform'),\n",
        "            'colsample_bytree': Real(0.5, 1.0, prior='uniform'),\n",
        "            'gamma': Real(0, 5, prior='uniform'), # Min loss reduction\n",
        "            'reg_alpha': Real(1e-3, 1e2, prior='log-uniform'), # L1 reg\n",
        "            'reg_lambda': Real(1e-3, 1e2, prior='log-uniform') # L2 reg\n",
        "        }\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'model': lgb.LGBMRegressor(random_state=SEED, n_jobs=-1),\n",
        "        'param_space': {\n",
        "            'n_estimators': Integer(100, 1500),\n",
        "            'learning_rate': Real(0.01, 0.2, prior='log-uniform'),\n",
        "            'num_leaves': Integer(10, 100),\n",
        "            'max_depth': Integer(5, 30),\n",
        "            'subsample': Real(0.5, 1.0, prior='uniform'),\n",
        "            'colsample_bytree': Real(0.5, 1.0, prior='uniform'),\n",
        "            'reg_alpha': Real(1e-3, 1e2, prior='log-uniform'),\n",
        "            'reg_lambda': Real(1e-3, 1e2, prior='log-uniform')\n",
        "        }\n",
        "    }\n",
        "    # Add Deep Learning models here requires separate handling within the loop\n",
        "    # Add GaussianProcessRegressor if dataset size permits (computationally heavy)\n",
        "}\n",
        "\n",
        "print(f\"Defined {len(models_to_evaluate)} candidate models/parameter spaces.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-phd-step7",
      "metadata": {
        "id": "markdown-phd-step7"
      },
      "source": [
        "## 7. Advanced Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-phd-step7-hpo",
      "metadata": {
        "id": "code-phd-step7-hpo"
      },
      "outputs": [],
      "source": [
        "print(\"--- 7. Hyperparameter Optimization Strategy ---\")\n",
        "print(f\"Using Bayesian Optimization (BayesSearchCV) with {N_ITER_BAYES} iterations within the inner CV loop.\")\n",
        "# The actual optimization happens inside the nested CV loop in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-phd-step8",
      "metadata": {
        "id": "markdown-phd-step8"
      },
      "source": [
        "## 8. Model Training & Evaluation (Nested CV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-phd-step8-nestedcv",
      "metadata": {
        "id": "code-phd-step8-nestedcv"
      },
      "outputs": [],
      "source": [
        "print(\"--- 8. Executing Nested Cross-Validation --- \")\n",
        "\n",
        "outer_results = {}\n",
        "best_models_per_fold = {}\n",
        "oof_predictions = {} # Store out-of-fold predictions for ensembling/analysis\n",
        "oof_indices = {}\n",
        "\n",
        "# Define parameters for feature engineering and selection to test\n",
        "feature_engineering_method = None # Or 'polynomial', 'fft_enhanced', etc. based on exploration\n",
        "feature_selection_method = 'lasso' # Or 'rfe', 'mutual_info', None\n",
        "n_features_to_select = 50 # If using selection method requiring N\n",
        "\n",
        "# --- Outer Loop ---\n",
        "for i, (train_outer_idx, test_outer_idx) in enumerate(outer_cv.split(X, y, groups=cv_groups)):\n",
        "    print(f\"\\n--- Outer Fold {i+1}/{N_SPLITS_OUTER} ---\")\n",
        "    X_train_outer, X_test_outer = X.iloc[train_outer_idx], X.iloc[test_outer_idx]\n",
        "    y_train_outer, y_test_outer = y[train_outer_idx], y[test_outer_idx]\n",
        "    groups_outer_train = groups[train_outer_idx] if groups is not None else None\n",
        "\n",
        "    # Store indices for OOF predictions\n",
        "    current_fold_indices = X.iloc[test_outer_idx].index.tolist()\n",
        "\n",
        "    # 1. Feature Engineering (Applied to this specific outer fold)\n",
        "    # X_train_outer_eng, X_test_outer_eng = engineer_features(X_train_outer, X_test_outer)\n",
        "    # For now, using original features + placeholder for selection\n",
        "    X_train_outer_eng, X_test_outer_eng = X_train_outer, X_test_outer\n",
        "\n",
        "    # 2. Feature Selection (Applied to this outer fold)\n",
        "    # X_train_outer_sel, X_test_outer_sel = select_features(\n",
        "    #     X_train_outer_eng, y_train_outer, X_test_outer_eng,\n",
        "    #     method=feature_selection_method, n_features=n_features_to_select\n",
        "    # )\n",
        "    # Using engineered (currently same as original) features for now\n",
        "    X_train_outer_sel, X_test_outer_sel = X_train_outer_eng, X_test_outer_eng\n",
        "\n",
        "    # 3. Scaling (Fit on this outer train fold)\n",
        "    # scaler = StandardScaler() # Or RobustScaler()\n",
        "    scaler = RobustScaler() # Good choice given potential outliers\n",
        "    X_train_outer_scaled = scaler.fit_transform(X_train_outer_sel)\n",
        "    X_test_outer_scaled = scaler.transform(X_test_outer_sel)\n",
        "    print(f\" Fold {i+1}: Data shapes after FeatEng/Select/Scale - Train: {X_train_outer_scaled.shape}, Test: {X_test_outer_scaled.shape}\")\n",
        "\n",
        "    outer_results[f'Fold_{i+1}'] = {}\n",
        "    best_models_per_fold[f'Fold_{i+1}'] = {}\n",
        "    oof_predictions[f'Fold_{i+1}'] = {}\n",
        "    oof_indices[f'Fold_{i+1}'] = current_fold_indices\n",
        "\n",
        "    # --- Inner Loop (Hyperparameter Tuning) ---\n",
        "    for model_name, config in models_to_evaluate.items():\n",
        "        print(f\"  Tuning {model_name}...\")\n",
        "        model = config['model']\n",
        "        param_space = config['param_space']\n",
        "\n",
        "        start_tune_time = time.time()\n",
        "\n",
        "        if param_space: # Use Bayesian Optimization if space is defined\n",
        "            # Need to ensure inner split uses groups correctly if outer uses them\n",
        "            inner_cv_fold = inner_cv.split(X_train_outer_scaled, y_train_outer, groups=groups_outer_train)\n",
        "\n",
        "            opt = BayesSearchCV(\n",
        "                estimator=model,\n",
        "                search_spaces=param_space,\n",
        "                scoring=scoring,\n",
        "                cv=inner_cv_fold, # Pass the generator\n",
        "                n_iter=N_ITER_BAYES,\n",
        "                random_state=SEED,\n",
        "                n_jobs=-1, # Use multiple cores\n",
        "                verbose=0 # Set to 1 for more details\n",
        "            )\n",
        "            try:\n",
        "               opt.fit(X_train_outer_scaled, y_train_outer) # Fit the optimizer\n",
        "               best_model_inner = opt.best_estimator_\n",
        "               best_score_inner = opt.best_score_\n",
        "            except Exception as e:\n",
        "               print(f\"    ERROR tuning {model_name}: {e}. Skipping.\")\n",
        "               best_model_inner = model # Fallback to default\n",
        "               best_score_inner = -np.inf # Indicate failure\n",
        "\n",
        "        elif hasattr(model, 'fit') and hasattr(model, 'predict'): # Models like RidgeCV/LassoCV handle their own tuning\n",
        "             try:\n",
        "                 model.fit(X_train_outer_scaled, y_train_outer)\n",
        "                 best_model_inner = model # The fitted model is the 'best'\n",
        "                 # Estimate inner score (approximate, as CV is internal)\n",
        "                 y_pred_inner = best_model_inner.predict(X_train_outer_scaled)\n",
        "                 best_score_inner = scoring._score_func(y_train_outer, y_pred_inner) # Score on training set (not ideal)\n",
        "             except Exception as e:\n",
        "                print(f\"    ERROR fitting {model_name}: {e}. Skipping.\")\n",
        "                best_model_inner = model # Fallback\n",
        "                best_score_inner = -np.inf # Indicate failure\n",
        "        else:\n",
        "            print(f\"    Skipping tuning for {model_name} (no param space or not applicable).\")\n",
        "            best_model_inner = model # Use default\n",
        "            best_score_inner = -np.inf # Indicate no tuning done\n",
        "\n",
        "        tune_time = time.time() - start_tune_time\n",
        "\n",
        "        # 4. Evaluate the *best model found in the inner loop* on the outer test set\n",
        "        if best_score_inner > -np.inf: # Check if tuning/fitting was successful\n",
        "           y_pred_outer = best_model_inner.predict(X_test_outer_scaled)\n",
        "           outer_mse = mean_squared_error(y_test_outer, y_pred_outer)\n",
        "           outer_rmse = np.sqrt(outer_mse)\n",
        "           outer_mae = mean_absolute_error(y_test_outer, y_pred_outer)\n",
        "           outer_r2 = r2_score(y_test_outer, y_pred_outer)\n",
        "\n",
        "           print(f\"    {model_name} - Outer Test RMSE: {outer_rmse:.4f}, R2: {outer_r2:.4f} (Tuning Time: {tune_time:.1f}s)\")\n",
        "           outer_results[f'Fold_{i+1}'][model_name] = {'RMSE': outer_rmse, 'MAE': outer_mae, 'R2': outer_r2}\n",
        "           best_models_per_fold[f'Fold_{i+1}'][model_name] = best_model_inner\n",
        "           oof_predictions[f'Fold_{i+1}'][model_name] = y_pred_outer\n",
        "        else:\n",
        "            print(f\"    {model_name} - Skipped evaluation due to tuning/fitting error.\")\n",
        "            outer_results[f'Fold_{i+1}'][model_name] = {'RMSE': np.nan, 'MAE': np.nan, 'R2': np.nan}\n",
        "\n",
        "    # --- Optional: Train/Evaluate Deep Learning Model Separately within Outer Fold ---\n",
        "    # DL models often require different HPO (Keras Tuner) and training procedures\n",
        "    # Placeholder:\n",
        "    print(\"  Placeholder for DL model training/evaluation within this outer fold...\")\n",
        "    # 1. Reshape data: X_train_dl = X_train_outer_scaled.reshape(...), X_test_dl = X_test_outer_scaled.reshape(...)\n",
        "    # 2. Define Keras Tuner search (e.g., Hyperband, BayesianOptimization)\n",
        "    # 3. tuner.search(X_train_dl, y_train_outer, validation_split=0.2, callbacks=[...])\n",
        "    # 4. best_dl_model = tuner.get_best_models(num_models=1)[0]\n",
        "    # 5. y_pred_outer_dl = best_dl_model.predict(X_test_dl)\n",
        "    # 6. Calculate metrics (RMSE, MAE, R2)\n",
        "    # 7. Store results in outer_results, best_models_per_fold, oof_predictions\n",
        "\n",
        "print(\"\\nNested Cross-Validation finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-phd-step8b-results",
      "metadata": {
        "id": "code-phd-step8b-results"
      },
      "outputs": [],
      "source": [
        "# --- Aggregate Nested CV Results ---\n",
        "aggregated_results = {}\n",
        "for model_name in models_to_evaluate.keys(): # Add DL model name if implemented\n",
        "    model_results = {'RMSE': [], 'MAE': [], 'R2': []}\n",
        "    for fold in outer_results:\n",
        "        if model_name in outer_results[fold]:\n",
        "            model_results['RMSE'].append(outer_results[fold][model_name]['RMSE'])\n",
        "            model_results['MAE'].append(outer_results[fold][model_name]['MAE'])\n",
        "            model_results['R2'].append(outer_results[fold][model_name]['R2'])\n",
        "        # else: Handle case where model failed in a fold\n",
        "\n",
        "    if model_results['RMSE']: # If results exist for this model\n",
        "       aggregated_results[model_name] = {\n",
        "           'RMSE_mean': np.nanmean(model_results['RMSE']),\n",
        "           'RMSE_std': np.nanstd(model_results['RMSE']),\n",
        "           'MAE_mean': np.nanmean(model_results['MAE']),\n",
        "           'R2_mean': np.nanmean(model_results['R2'])\n",
        "       }\n",
        "\n",
        "nested_cv_results_df = pd.DataFrame(aggregated_results).T\n",
        "nested_cv_results_df = nested_cv_results_df.sort_values(by='RMSE_mean', ascending=True)\n",
        "\n",
        "print(\"\\n--- Aggregated Nested Cross-Validation Results (Mean +/- Std across outer folds) ---\")\n",
        "display(nested_cv_results_df.round(4))\n",
        "\n",
        "# --- Reconstruct OOF Predictions ---\n",
        "oof_preds_final = {}\n",
        "true_values_oof = pd.Series(index=X.index, dtype=float)\n",
        "model_names_with_preds = list(models_to_evaluate.keys()) # Add DL model name\n",
        "\n",
        "for model_name in model_names_with_preds:\n",
        "    oof_preds_final[model_name] = pd.Series(index=X.index, dtype=float)\n",
        "\n",
        "for fold in oof_indices:\n",
        "    indices = oof_indices[fold]\n",
        "    true_values_oof.loc[indices] = y[indices] # Store true y for these indices\n",
        "    for model_name in model_names_with_preds:\n",
        "        if model_name in oof_predictions[fold]:\n",
        "             oof_preds_final[model_name].loc[indices] = oof_predictions[fold][model_name].flatten()\n",
        "\n",
        "# Drop rows where prediction might be missing (if a model failed in a fold)\n",
        "oof_df = pd.DataFrame(oof_preds_final)\n",
        "oof_df['True'] = true_values_oof\n",
        "oof_df.dropna(inplace=True) # Drop rows where any model failed\n",
        "\n",
        "print(f\"\\nReconstructed Out-of-Fold (OOF) predictions for {len(oof_df)} instances.\")\n",
        "# Can calculate overall OOF metrics here\n",
        "print(\"Overall OOF Performance (calculated on combined OOF predictions):\")\n",
        "oof_metrics = {}\n",
        "for model_name in model_names_with_preds:\n",
        "     if model_name in oof_df.columns:\n",
        "        rmse = np.sqrt(mean_squared_error(oof_df['True'], oof_df[model_name]))\n",
        "        r2 = r2_score(oof_df['True'], oof_df[model_name])\n",
        "        oof_metrics[model_name] = {'OOF_RMSE': rmse, 'OOF_R2': r2}\n",
        "        print(f\"  {model_name}: OOF RMSE={rmse:.4f}, OOF R2={r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-phd-step9",
      "metadata": {
        "id": "markdown-phd-step9"
      },
      "source": [
        "## 9. Ensemble Methods & Final Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-phd-step9-ensemble",
      "metadata": {
        "id": "code-phd-step9-ensemble"
      },
      "outputs": [],
      "source": [
        "print(\"--- 9. Ensemble Methods (using OOF predictions) ---\")\n",
        "\n",
        "# Use the OOF predictions (oof_df) as input features for a meta-learner (Stacking)\n",
        "# or simply average/weight the best models (Blending/Voting).\n",
        "\n",
        "# --- 9a. Blending/Voting (Simpler) ---\n",
        "# Select top N models based on OOF performance or CV results\n",
        "top_models = nested_cv_results_df.head(3).index.tolist()\n",
        "print(f\"Selected top models for blending: {top_models}\")\n",
        "\n",
        "if len(top_models) > 1:\n",
        "    # Simple Averaging Blend\n",
        "    oof_df['Blend_Avg'] = oof_df[top_models].mean(axis=1)\n",
        "    blend_rmse = np.sqrt(mean_squared_error(oof_df['True'], oof_df['Blend_Avg']))\n",
        "    blend_r2 = r2_score(oof_df['True'], oof_df['Blend_Avg'])\n",
        "    print(f\"  Simple Average Blend: OOF RMSE={blend_rmse:.4f}, OOF R2={blend_r2:.4f}\")\n",
        "\n",
        "    # Weighted Average Blend (e.g., based on inverse RMSE)\n",
        "    weights = 1 / nested_cv_results_df.loc[top_models, 'RMSE_mean']\n",
        "    weights = weights / weights.sum() # Normalize\n",
        "    oof_df['Blend_Weighted'] = np.average(oof_df[top_models], axis=1, weights=weights)\n",
        "    blend_w_rmse = np.sqrt(mean_squared_error(oof_df['True'], oof_df['Blend_Weighted']))\n",
        "    blend_w_r2 = r2_score(oof_df['True'], oof_df['Blend_Weighted'])\n",
        "    print(f\"  Weighted Average Blend: OOF RMSE={blend_w_rmse:.4f}, OOF R2={blend_w_r2:.4f}\")\n",
        "\n",
        "# --- 9b. Stacking ---\n",
        "# Train a meta-model (e.g., Ridge) on OOF predictions\n",
        "print(\"\\nSetting up Stacking (conceptual - requires final model training):\")\n",
        "# 1. Select base models (can be different from top_models)\n",
        "base_models_stacking = [(name, best_models_per_fold[f'Fold_{N_SPLITS_OUTER-1}'][name])\n",
        "                        for name in top_models if name in best_models_per_fold[f'Fold_{N_SPLITS_OUTER-1}']]\n",
        "# NOTE: This uses models trained on the LAST fold for illustration. A proper approach\n",
        "#       would retrain final models on full dataset or use all fold models.\n",
        "\n",
        "if len(base_models_stacking) > 1:\n",
        "   # Define Meta Learner\n",
        "   meta_learner = RidgeCV()\n",
        "\n",
        "   # Stacking Regressor (conceptual setup)\n",
        "   # stack = StackingRegressor(estimators=base_models_stacking, final_estimator=meta_learner, cv='prefit') # Needs careful handling of prefit models\n",
        "   # A common approach is to train the meta-learner directly on the oof_df predictions\n",
        "   X_meta_train = oof_df[top_models]\n",
        "   y_meta_train = oof_df['True']\n",
        "\n",
        "   meta_learner.fit(X_meta_train, y_meta_train)\n",
        "   y_pred_stack_oof = meta_learner.predict(X_meta_train)\n",
        "   stack_rmse_oof = np.sqrt(mean_squared_error(y_meta_train, y_pred_stack_oof))\n",
        "   stack_r2_oof = r2_score(y_meta_train, y_pred_stack_oof)\n",
        "   print(f\"  Stacking (Meta-Learner trained on OOF): OOF RMSE={stack_rmse_oof:.4f}, OOF R2={stack_r2_oof:.4f}\")\n",
        "   # To get final predictions on new data, base models predict first, then meta-learner predicts on those outputs.\n",
        "else:\n",
        "    print(\" Not enough base models available for stacking example.\")\n",
        "\n",
        "# --- Final Model Selection ---\n",
        "# Choose the best single model or ensemble based on OOF performance and stability (CV std dev).\n",
        "print(\"\\nFinal Model: Select based on overall OOF performance (e.g., lowest RMSE from individual models or ensembles).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-phd-step10",
      "metadata": {
        "id": "markdown-phd-step10"
      },
      "source": [
        "## 10. Model Interpretation & Explainability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-phd-step10-shap",
      "metadata": {
        "id": "code-phd-step10-shap"
      },
      "outputs": [],
      "source": [
        "print(\"--- 10. Model Interpretation (Example using SHAP) ---\")\n",
        "\n",
        "# Select the best performing model (or a tree-based model for easier SHAP interpretation)\n",
        "# Retrain the final chosen model on the full training dataset (or use one from the last fold for illustration)\n",
        "final_model_name = nested_cv_results_df.index[0] # Example: Best model from CV\n",
        "print(f\"Interpreting model: {final_model_name}\")\n",
        "\n",
        "# Need data in the format the model was trained on (after FE, FS, Scaling)\n",
        "# Retrain final pipeline steps on full X, y (or use last fold's components for illustration)\n",
        "# Placeholder: Assume we have a final fitted model 'final_model' and corresponding scaled full training data 'X_scaled_full'\n",
        "# Example: Using the model from the last fold and the corresponding training data\n",
        "try:\n",
        "    last_fold_idx = N_SPLITS_OUTER - 1\n",
        "    last_fold_key = f'Fold_{last_fold_idx+1}'\n",
        "    final_model = best_models_per_fold[last_fold_key][final_model_name]\n",
        "\n",
        "    # Re-run preprocessing steps on the *training data* of the last fold to get the input for SHAP\n",
        "    train_outer_idx_last, _ = list(outer_cv.split(X, y, groups=cv_groups))[last_fold_idx]\n",
        "    X_train_outer_last = X.iloc[train_outer_idx_last]\n",
        "    y_train_outer_last = y[train_outer_idx_last]\n",
        "    # Apply FE/FS/Scaling exactly as done in that fold's training\n",
        "    # Placeholder for simplicity - using the scaled data from that fold\n",
        "    # Need to regenerate X_train_outer_scaled from the loop above for the last fold\n",
        "    # This part requires careful management of the processing pipeline state from the CV loop\n",
        "    print(\"SHAP requires correctly processed data matching the model's training input.\")\n",
        "    print(\"Conceptual SHAP application:\")\n",
        "\n",
        "    # Choose appropriate explainer based on model type\n",
        "    if isinstance(final_model, (RandomForestRegressor, GradientBoostingRegressor, xgb.XGBRegressor, lgb.LGBMRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor)):\n",
        "        explainer = shap.TreeExplainer(final_model)\n",
        "        # Need the corresponding X_train_outer_scaled for this fold\n",
        "        # shap_values = explainer.shap_values(X_train_outer_scaled) # Placeholder data\n",
        "    elif isinstance(final_model, (RidgeCV, LassoCV, SVR, KNeighborsRegressor)):\n",
        "        # Use KernelExplainer (slower) - requires a background dataset\n",
        "        # background_data = shap.sample(X_train_outer_scaled, 100) # Sample background data\n",
        "        # explainer = shap.KernelExplainer(final_model.predict, background_data)\n",
        "        # shap_values = explainer.shap_values(X_test_outer_scaled_from_last_fold) # Example on test data\n",
        "        print(f\"SHAP for {final_model_name} might require KernelExplainer (slower).\")\n",
        "    else:\n",
        "        print(f\"SHAP explainer not straightforwardly defined for {type(final_model)}.\")\n",
        "\n",
        "    # If SHAP values were calculated:\n",
        "    # shap.summary_plot(shap_values, X_train_outer_scaled, feature_names=X_train_outer_sel.columns) # Use correct data/features\n",
        "    # shap.summary_plot(shap_values, X_train_outer_scaled, plot_type='bar', feature_names=X_train_outer_sel.columns)\n",
        "    # Can also plot dependence plots: shap.dependence_plot('feature_name', shap_values, X_train_outer_scaled)\n",
        "except KeyError as e:\n",
        "     print(f\"Could not retrieve model/data for SHAP for {final_model_name} from last fold: {e}\")\n",
        "except Exception as e:\n",
        "     print(f\"An error occurred during SHAP setup: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "markdown-phd-step11",
      "metadata": {
        "id": "markdown-phd-step11"
      },
      "source": [
        "## 11. Reporting, Discussion & Future Work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-phd-step11-report",
      "metadata": {
        "id": "code-phd-step11-report"
      },
      "outputs": [],
      "source": [
        "print(\"--- 11. Final Report Summary ---\")\n",
        "\n",
        "display(Markdown(\"### Final Results Summary\"))\n",
        "print(\"Nested CV Aggregated Performance:\")\n",
        "display(nested_cv_results_df.round(4))\n",
        "print(\"\\nOverall OOF Performance (using combined predictions):\")\n",
        "display(pd.DataFrame(oof_metrics).T.round(4))\n",
        "# Add Ensemble OOF results if calculated\n",
        "\n",
        "display(Markdown(\"### Discussion\"))\n",
        "display(Markdown(f\"\"\"\n",
        "*   **Best Performing Approach:** Based on Nested CV (RMSE mean: {nested_cv_results_df.iloc[0]['RMSE_mean']:.4f} +/- {nested_cv_results_df.iloc[0]['RMSE_std']:.4f}) and OOF validation (RMSE: {oof_metrics.get(nested_cv_results_df.index[0], {'OOF_RMSE': np.nan})['OOF_RMSE']:.4f}), the best approach identified in this analysis was **{nested_cv_results_df.index[0]}**.\n",
        "*   **Impact of Feature Engineering/Selection:** Discuss which FE/FS choices were tested and their apparent impact (e.g., 'Using Lasso selection resulted in slightly better performance for tree models compared to no selection...').\n",
        "*   **Model Comparison:** Compare the performance of different model families (linear, trees, SVM, DL). Were tree ensembles consistently better? How did LSTM fare?\n",
        "*   **Ensemble Performance:** Did blending or stacking provide a significant improvement over the best single model?\n",
        "*   **Interpretability:** Summarize findings from SHAP analysis (if performed). Which features consistently drove predictions? Do they align with clinical knowledge or findings from the literature review?\n",
        "\"\"\"))\n",
        "\n",
        "display(Markdown(\"### Limitations\"))\n",
        "display(Markdown(\"\"\"\n",
        "*   **Pre-extracted Features:** The primary limitation is the lack of raw audio data. Signal processing features (FFT, Wavelets) were applied to the *feature time series*, not the original voice signal, limiting their physical interpretability and potential effectiveness compared to analysis of raw audio.\n",
        "*   **Dataset Size/Diversity:** The dataset size and number of subjects might limit the generalizability of findings.\n",
        "*   **Feature Engineering Scope:** The tested FE techniques represent a subset of possibilities.\n",
        "*   **Hyperparameter Search Space:** The Bayesian optimization explored a defined space; wider or different spaces might yield better results.\n",
        "*   **Computational Resources:** Exhaustive search across all combinations of FE, FS, HPO is computationally prohibitive.\n",
        "*   **Validation Strategy:** While Nested CV with GroupKFold is robust, it assumes subject labels are correct and fully capture dependencies.\n",
        "\"\"\"))\n",
        "\n",
        "display(Markdown(\"### Future Work\"))\n",
        "display(Markdown(\"\"\"\n",
        "*   **Acquire Raw Audio:** If possible, repeating the analysis starting from raw audio signals would be the most impactful next step, allowing for true signal processing FE.\n",
        "*   **Explore More Features:** Investigate other feature types (e.g., fractal dimensions, entropy measures, phoneme-specific features if transcription possible).\n",
        "*   **Advanced Deep Learning:** Experiment with more complex DL architectures (CNN-LSTM, Transformers, Attention mechanisms) if computationally feasible.\n",
        "*   **Refined Subject Handling:** If more detailed subject metadata were available (e.g., demographics, medication status), incorporate it into the analysis or stratified validation.\n",
        "*   **External Validation:** Test the final selected model on an independent Parkinson's voice dataset if available.\n",
        "*   **Causality Analysis:** Explore causal inference techniques to understand feature relationships beyond correlation (requires strong assumptions).\n",
        "*   **Longitudinal Analysis:** If data represented multiple recordings over time *for the same individuals*, model the progression trajectory rather than just a single score prediction.\n",
        "\"\"\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}